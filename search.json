[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tyler Grimes",
    "section": "",
    "text": "Google Scholar\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Twitter\n  \n  \n    \n     Github\n  \n\n      \nBiostatistician working in genomics and clinical research, currently lead analyst for studies of immunologic response to disease and vaccines through the analysis of transcriptomics, genomics, proteomics, metabolomics, and lipidomics data. Highly skilled in developing bioinformatics analysis workflows, building predictive models, and designing simulation studies. Key role is translating statistical results and connecting the findings to the biological question at hand."
  },
  {
    "objectID": "posts/2023-10-28-overfitting/index.html",
    "href": "posts/2023-10-28-overfitting/index.html",
    "title": "Overfitting in the search for biomarkers",
    "section": "",
    "text": "This post will be about overfitting in high-dimensional models using -omics measure to predict clinical endpoints.\n\nn = 100\np = 50\nx = matrix(rnorm(n * p), nrow = n, ncol = p)\nhist(x[, 1])\n\n\n\nplot(x[, 1], x[, 2])"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nPredictive modeling with differential co-expression of RNA-seq data\n\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\n\n\n\n\n  \n\n\n\n\nIdentifying cancer driver genes from differential co-expression networks\n\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\nAn exploratory approach for identifying novel biomarkers in subgroups of cancer patients from RNA-seq data\n\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2018\n\n\n\n\n\n\n  \n\n\n\n\nPredicting survival times for neuroblastoma patients using RNA-Seq expression profiles\n\n\n\n\n\n\n\n\n\n\n\n\nJul 23, 2017\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/2023_BMC_Bio_pseudo/index.html",
    "href": "publications/2023_BMC_Bio_pseudo/index.html",
    "title": "A pseudo-value regression approach for differential network analysis of co-expression data",
    "section": "",
    "text": "In BMC Bioinformatics"
  },
  {
    "objectID": "publications/2023_BMC_Bio_pseudo/index.html#abstract",
    "href": "publications/2023_BMC_Bio_pseudo/index.html#abstract",
    "title": "A pseudo-value regression approach for differential network analysis of co-expression data",
    "section": "Abstract",
    "text": "Abstract\nBackground: The differential network (DN) analysis identifies changes in measures of association among genes under two or more experimental conditions. In this article, we introduce a pseudo-value regression approach for network analysis (PRANA). This is a novel method of differential network analysis that also adjusts for additional clinical covariates. We start from mutual information criteria, followed by pseudo-value calculations, which are then entered into a robust regression model.\nResults This article assesses the model performances of PRANA in a multivariable setting, followed by a comparison to dnapath and DINGO in both univariable and multivariable settings through variety of simulations. Performance in terms of precision, recall, and F1 score of differentially connected (DC) genes is assessed. By and large, PRANA outperformed dnapath and DINGO, neither of which is equipped to adjust for available covariates such as patient-age. Lastly, we employ PRANA in a real data application from the Gene Expression Omnibus database to identify DC genes that are associated with chronic obstructive pulmonary disease to demonstrate its utility.\nConclusion To the best of our knowledge, this is the first attempt of utilizing a regression modeling for DN analysis by collective gene expression levels between two or more groups with the inclusion of additional clinical covariates. By and large, adjusting for available covariates improves accuracy of a DN analysis."
  },
  {
    "objectID": "publications/2017_camda_neuroblastoma/index.html",
    "href": "publications/2017_camda_neuroblastoma/index.html",
    "title": "Predicting survival times for neuroblastoma patients using RNA-seq expression profiles",
    "section": "",
    "text": "In Biology Direct"
  },
  {
    "objectID": "publications/2017_camda_neuroblastoma/index.html#abstract",
    "href": "publications/2017_camda_neuroblastoma/index.html#abstract",
    "title": "Predicting survival times for neuroblastoma patients using RNA-seq expression profiles",
    "section": "Abstract",
    "text": "Abstract\nBackground: Neuroblastoma is the most common tumor of early childhood and is notorious for its high variability in clinical presentation. Accurate prognosis has remained a challenge for many patients. In this study, expression profiles from RNA-sequencing are used to predict survival times directly. Several models are investigated using various annotation levels of expression profiles (genes, transcripts, and introns), and an ensemble predictor is proposed as a heuristic for combining these different profiles.\nResults: The use of RNA-seq data is shown to improve accuracy in comparison to using clinical data alone for predicting overall survival times. Furthermore, clinically high-risk patients can be subclassified based on their predicted overall survival times. In this effort, the best performing model was the elastic net using both transcripts and introns together. This model separated patients into two groups with 2-year overall survival rates of 0.40±0.11 (n=22) versus 0.80±0.05 (n=68). The ensemble approach gave similar results, with groups 0.42±0.10 (n=25) versus 0.82±0.05 (n=65). This suggests that the ensemble is able to effectively combine the individual RNA-seq datasets. \nConclusions: Using predicted survival times based on RNA-seq data can provide improved prognosis by subclassifying clinically high-risk neuroblastoma patients."
  },
  {
    "objectID": "publications/2017_camda_neuroblastoma/index.html#summary",
    "href": "publications/2017_camda_neuroblastoma/index.html#summary",
    "title": "Predicting survival times for neuroblastoma patients using RNA-seq expression profiles",
    "section": "Summary",
    "text": "Summary\nIn this study, expression profiles from RNA-sequencing of Neuroblastoma tumor samples are used to predict patient survival times. Several models are investigated using various annotation levels of expression profiles (genes, transcripts, and introns), and an ensemble predictor is proposed as a heuristic for combining these different profiles. The use of RNA-seq data is shown to improve accuracy in comparison to using clinical data alone for predicting overall survival times.\nSource code Slides"
  },
  {
    "objectID": "publications/2017_camda_metasub/index.html",
    "href": "publications/2017_camda_metasub/index.html",
    "title": "Unraveling bacterial fingerprints of city subways from microbiome 16S gene profiles",
    "section": "",
    "text": "In Biology Direct"
  },
  {
    "objectID": "publications/2017_camda_metasub/index.html#abstract",
    "href": "publications/2017_camda_metasub/index.html#abstract",
    "title": "Unraveling bacterial fingerprints of city subways from microbiome 16S gene profiles",
    "section": "Abstract",
    "text": "Abstract\nBackground: Microbial communities can be location specific, and the abundance of species within locations can influence our ability to determine whether a sample belongs to one city or another. As part of the 2017 CAMDA MetaSUB Inter-City Challenge, next generation sequencing (NGS) data was generated from swipe samples collected from subway stations in Boston, New York City hereafter New York, and Sacramento. DNA was extracted and Illumina sequenced. Sequencing data was provided for all cities as part of 2017 CAMDA contest challenge dataset.\nResults: Principal component analysis (PCA) showed clear clustering of the samples for the three cities, with a substantial proportion of the variance explained by the first three components. We ran two different classifiers and results were robust for error rate (< 6%) and accuracy (> 95%). The analysis of variance (ANOVA) demonstrated that overall, bacterial composition across the three cities is significantly different. A similar conclusion was reached using a novel bootstrap based test using diversity indices. Last but not least, a co-abundance association network analyses for the taxonomic levels “order”, “family”, and “genus” found different patterns of bacterial networks for the three cities.\nConclusions: Bacterial fingerprint can be useful to predict sample provenance. In this work prediction of provenance reported with over 95% accuracy. Association based network analysis, emphasized similarities between the closest cities sharing common bacterial composition. ANOVA showed different patterns of bacterial amongst cities, and these findings strongly suggest that bacterial signature across multiple cities are different. This work advocates a data analysis pipeline which could be followed in order to get biological insight from this data. However, the biological conclusions from this analysis is just an early indication out of a pilot microbiome data provided to us through CAMDA 2017 challenge and will be subject to change as we get more complete data sets in the near future. This microbiome data can have potential applications in forensics, ecology, and other sciences.” abstract_short = “As part of the 2017 CAMDA MetaSUB Inter-City Challenge, next generation sequencing (NGS) data was generated from swipe samples collected from subway stations in Boston, New York City hereafter New York, and Sacramento. This work advocates a data analysis pipeline that could be followed in order to get biological insight from this data."
  },
  {
    "objectID": "publications/2021_plosone_prob/index.html",
    "href": "publications/2021_plosone_prob/index.html",
    "title": "A novel probabilistic generator for large-scale gene association networks",
    "section": "",
    "text": "In PLOS ONE"
  },
  {
    "objectID": "publications/2021_plosone_prob/index.html#abstract",
    "href": "publications/2021_plosone_prob/index.html#abstract",
    "title": "A novel probabilistic generator for large-scale gene association networks",
    "section": "Abstract",
    "text": "Abstract\nMotivation: Gene expression data provide an opportunity for reverse-engineering gene-gene associations using network inference methods. However, it is difficult to assess the performance of these methods because the true underlying network is unknown in real data. Current benchmarks address this problem by subsampling a known regulatory network to conduct simulations. But the topology of regulatory networks can vary greatly across organisms or tissues, and reference-based generators—such as GeneNetWeaver—are not designed to capture this heterogeneity. This means, for example, benchmark results from the E. coli regulatory network will not carry over to other organisms or tissues. In contrast, probabilistic generators do not require a reference network, and they have the potential to capture a rich distribution of topologies. This makes probabilistic generators an ideal approach for obtaining a robust benchmarking of network inference methods.\nResults: We propose a novel probabilistic network generator that (1) provides an alternative to address the inherent limitation of reference-based generators and (2) is able to create realistic gene association networks, and (3) captures the heterogeneity found across gold-standard networks better than existing generators used in practice. Eight organism-specific and 12 human tissue-specific gold-standard association networks are considered. Several measures of global topology are used to determine the similarity of generated networks to the gold-standards. Along with demonstrating the variability of network structure across organisms and tissues, we show that the commonly used “scale-free” model is insufficient for replicating these structures."
  },
  {
    "objectID": "publications/2021_plosone_prob/index.html#summary",
    "href": "publications/2021_plosone_prob/index.html#summary",
    "title": "A novel probabilistic generator for large-scale gene association networks",
    "section": "Summary",
    "text": "Summary\nA novel probabilistic network generator is proposed that (1) provides an alternative to address the inherent limitation of reference-based generators and (2) is able to create realistic gene association networks, and (3) captures the heterogeneity found across gold-standard networks better than existing generators used in practice. Eight organism-specific and 12 human tissue-specific gold-standard association networks are considered. Several measures of global topology are used to determine the similarity of generated networks to the gold-standards. Along with demonstrating the variability of network structure across organisms and tissues, we show that the commonly used “scale-free” model is insufficient for replicating these structures.\ndoi: https://doi.org/10.1371/journal.pone.0259193"
  },
  {
    "objectID": "publications/2021_frontiers_tumor_purity/index.html",
    "href": "publications/2021_frontiers_tumor_purity/index.html",
    "title": "The Analysis of Gene Expression Data Incorporating Tumor Purity Information",
    "section": "",
    "text": "In BMC Bioinformatics"
  },
  {
    "objectID": "publications/2021_frontiers_tumor_purity/index.html#abstract",
    "href": "publications/2021_frontiers_tumor_purity/index.html#abstract",
    "title": "The Analysis of Gene Expression Data Incorporating Tumor Purity Information",
    "section": "Abstract",
    "text": "Abstract\nThe tumor microenvironment is composed of tumor cells, stroma cells, immune cells, blood vessels, and other associated non-cancerous cells. Gene expression measurements on tumor samples are an average over cells in the microenvironment. However, research questions often seek answers about tumor cells rather than the surrounding non-tumor tissue. Previousstudies have suggested that the tumor purity (TP)—the proportion of tumor cells in a solid tumor sample—has a confounding effect on differential expression (DE) analysis of high vs. low survival groups. We investigate three ways incorporating the TP information in the two statistical methods used for analyzing gene expression data, namely, differential network (DN) analysis and DE analysis. Analysis 1 ignores the TP information completely, Analysis 2 uses a truncated sample by removing the low TP samples, and Analysis 3 uses TP as a covariate in the underlying statistical models. We use three gene expression data sets related to three different cancers from the Cancer Genome Atlas (TCGA) for our investigation. The networks from Analysis 2 have greater amount of differential connectivity in the two networks than that from Analysis 1 in all three cancer datasets. Similarly, Analysis 1 identified more differentially expressed genes than Analysis 2. Results of DN and DE analyses using Analysis 3 were mostly consistent with those of Analysis 1 across three cancers. However, Analysis 3 identified additional cancer-related genes in both DN and DE analyses. Our findings suggest that using TP as a covariate in a linear model is appropriate for DE analysis, but a more robust model is needed for DN analysis. However, because true DN or DE patterns are not known for the empirical datasets, simulated datasets can be used to study the statistical properties of these methods in future studies."
  },
  {
    "objectID": "publications/2019_pathways_in_dc/index.html",
    "href": "publications/2019_pathways_in_dc/index.html",
    "title": "Integrating gene regulatory pathways into differential network analysis of gene expression data",
    "section": "",
    "text": "In Scientific Reports"
  },
  {
    "objectID": "publications/2019_pathways_in_dc/index.html#abstract",
    "href": "publications/2019_pathways_in_dc/index.html#abstract",
    "title": "Integrating gene regulatory pathways into differential network analysis of gene expression data",
    "section": "Abstract",
    "text": "Abstract\nBackground: The analysis of gene-gene co-expression networks provides insight into the function of gene products. Exposing network irregularities offers an avenue for discovery in systems biology; these pursuits can include the study of gene function in developmental biology and understanding and treating diseases. Modern methods for differential network analysis often have two drawbacks: they implicitly rely on the selection of a relatively small subset of genes before analysis, and they are not flexible to the choice of association measure.\nMethods: A general framework for integrating known gene regulatory pathways into a differential network analysis is proposed. The framework allows for any gene-gene association measure to be used, and inference is carried out through permutation testing. A simulation study investigates the performance in identifying differentially connected genes when incorporating known pathways and compares the general framework to four state-of- the-art methods. Two RNA-seq datasets are analyzed to illustrate the use of this framework in practice.\nResults: The simulation study shows that incorporating pathway information can improve performance in terms of both sensitivity and true discovery rate. Furthermore, we demonstrate that the state-of-the-art methods each estimate different things and are not directly comparable – this emphasizes the fact that the choice of association measure can have a strong influence on results. In the applied examples, the analysis reveals genes and pathways that are known to be biologically significant along with new findings to motivate future research.\nConclusions: The proposed framework makes explicit two critical, but often overlooked, assumptions: the selection of a subset of genes and the meaning of gene-gene association. The results obtained from analyzing gene expression with this framework are more interpretable, and the pathway information provides context that can lead to deeper insights."
  },
  {
    "objectID": "publications/2019_pathways_in_dc/index.html#summary",
    "href": "publications/2019_pathways_in_dc/index.html#summary",
    "title": "Integrating gene regulatory pathways into differential network analysis of gene expression data",
    "section": "Summary",
    "text": "Summary\nA framework for comparing gene-gene associations between two populations is proposed. Any association measure may be used - correlations, partial correlation, mutual information, etc; this flexibility is in contrast to the rigidity of existing methods. The framework makes explicit the practice of incoporating pathway information into the analysis. A simulation study explores the affect this has on performance, and we investigate what happens when pathway information is misspecified or incomplete.\ndoi: 10.1038/s41598-019-41918-3\ndnapath R package available on CRAN"
  },
  {
    "objectID": "publications/2020_jss_seqnet/index.html",
    "href": "publications/2020_jss_seqnet/index.html",
    "title": "SeqNet: An R Package for Generating Gene-Gene Networks and Simulating RNA-Seq Data",
    "section": "",
    "text": "In Journal of Statistical Software"
  },
  {
    "objectID": "publications/2020_jss_seqnet/index.html#abstract",
    "href": "publications/2020_jss_seqnet/index.html#abstract",
    "title": "SeqNet: An R Package for Generating Gene-Gene Networks and Simulating RNA-Seq Data",
    "section": "Abstract",
    "text": "Abstract\nGene expression data provide an abundant resource for inferring connections in gene regulatory networks. While methodologies developed for this task have shown success, a challenge remains in comparing the performance among methods. Gold-standard datasets are scarce and limited in use. And while tools for simulating expression data are available, they are not designed to resemble the data obtained from RNA-seq experiments. SeqNet is an R package that provides tools for generating a rich variety of gene network structures and simulating RNA-seq data from them. This produces in silico RNA-seq data for benchmarking and assessing gene network inference methods. The package is available on CRAN and on GitHub at https://github.com/tgrimes/SeqNet."
  },
  {
    "objectID": "publications/2020_jss_seqnet/index.html#summary",
    "href": "publications/2020_jss_seqnet/index.html#summary",
    "title": "SeqNet: An R Package for Generating Gene-Gene Networks and Simulating RNA-Seq Data",
    "section": "Summary",
    "text": "Summary\nSeqNet is an R package that provides tools for generating a rich variety of gene network structures and simulating RNA-seq data from them. This produces in silico RNA-seq data for benchmarking and assessing gene network inference methods.\ndoi: 10.18637/jss.v098.i12\nSeqNet R package available on CRAN"
  },
  {
    "objectID": "talks/2017_ISMB/index.html",
    "href": "talks/2017_ISMB/index.html",
    "title": "Predicting survival times for neuroblastoma patients using RNA-Seq expression profiles",
    "section": "",
    "text": "My talk at the ISMB/CAMDA International Conference in Chicago, IL."
  },
  {
    "objectID": "talks/2017_ISMB/index.html#abstract",
    "href": "talks/2017_ISMB/index.html#abstract",
    "title": "Predicting survival times for neuroblastoma patients using RNA-Seq expression profiles",
    "section": "Abstract",
    "text": "Abstract\nGene expression profiles from RNA-sequencing provide a window to the activity inside tumor cells. This view has enabled researchers to identify driver genes for the pathology, but often these results fail to validate or only hold for the particular cancer at hand. In this study, we investigate a robust method for identifying cancer-related genes. Two criteria are used: a gene must show functional changes among clinically different groups of patients, and the gene must have clinical relevance. These conditions are assessed through a differential network analysis and by modelling patient survival times. Preliminary results are shown from a neuroblastoma and breast cancer dataset.\nEvent: ISMB/CAMDA 2017, Prague, Czech Republic\nSlides\n\nQ&A starts at 16:20"
  },
  {
    "objectID": "talks/2020_CMStatistics/index.html",
    "href": "talks/2020_CMStatistics/index.html",
    "title": "Identifying cancer driver genes from differential co-expression networks",
    "section": "",
    "text": "Invited talk at the CMStatistics 13th International Conference."
  },
  {
    "objectID": "talks/2020_CMStatistics/index.html#abstract",
    "href": "talks/2020_CMStatistics/index.html#abstract",
    "title": "Identifying cancer driver genes from differential co-expression networks",
    "section": "Abstract",
    "text": "Abstract\nThe underlying driver of cancer stems from somatic mutations in the genome that change the function of gene products. However, not all mutations are associated with cancer progression. Rather, such “passenger” mutations are a symptom of DNA instability. Only a small portion of mutations are actual “drivers” that are responsible for disease progression. A methodology is proposed for analyzing gene expression data, to identify driver genes by considering both the functional changes of genes and the clinical relevance of those changes. Functional changes are identified by performing a differential network analysis, which compares the structure of gene-gene associations across different stages of cancer. Genes that are differentially connected indicate a change in their functional activity along with cancer progression. Clinical relevance of these differential connections is assessed using a survival model to predict overall survival. Potential driver genes are identified for Neuroblastoma and Breast cancer patient populations. We identify regulatory pathways that are both differentially connected and whose expression profile is predictive of overall survival. We plan to analyze additional cancers from the TCGA data repository and perform a meta-analysis to identify any pan-cancer driver genes or biomarkers.\nEvent: CMStatistics 2020"
  },
  {
    "objectID": "talks/2022_UMBC/index.html",
    "href": "talks/2022_UMBC/index.html",
    "title": "Predictive modeling with differential co-expression of RNA-seq data",
    "section": "",
    "text": "Invited talk at the Statistical Colloquium for the University of Maryland, Department of Mathematics and Statistics."
  },
  {
    "objectID": "talks/2022_UMBC/index.html#abstract",
    "href": "talks/2022_UMBC/index.html#abstract",
    "title": "Predictive modeling with differential co-expression of RNA-seq data",
    "section": "Abstract",
    "text": "Abstract\nGene expression data from RNA-seq experiments provide a measure of biological activity at the cellular level. Differential expression (DE) analyses use these data to identify genes that are up- or down-regulated across groups of interest. Research in systems biology takes this analysis further by gleaning insights into gene regulatory networks through the analysis of gene co-expression networks. Similar to DE analysis, a differential network (DN) analysis is conducted to determine how co-expression patterns change across different groups of interest. A methodology is proposed for predictive modeling in this context that builds on a framework for conducting pathway-based differential network analysis. In an application to cancer gene expression datasets, our goal is to identify driver genes by considering both the functional changes of genes and the clinical relevance of those changes. Functional changes are identified by performing a differential network analysis, which compares the structure of gene-gene associations across different stages of cancer. Clinical relevance of these differential connections is assessed through predictive modeling. Potential driver genes are identified for Neuroblastoma and Breast cancer patient populations, and we identify regulatory pathways that are both differentially connected and whose expression profile is predictive of overall survival.\nEvent: Statistics Colloquium 2022\nSlides"
  },
  {
    "objectID": "talks/2018_ISMB/index.html",
    "href": "talks/2018_ISMB/index.html",
    "title": "An exploratory approach for identifying novel biomarkers in subgroups of cancer patients from RNA-seq data",
    "section": "",
    "text": "My talk at the ISMB/CAMDA International Conference in Chicago, IL."
  },
  {
    "objectID": "talks/2018_ISMB/index.html#abstract",
    "href": "talks/2018_ISMB/index.html#abstract",
    "title": "An exploratory approach for identifying novel biomarkers in subgroups of cancer patients from RNA-seq data",
    "section": "Abstract",
    "text": "Abstract\nGene expression profiles from RNA-sequencing provide a window to the activity inside tumor cells. This view has enabled researchers to identify driver genes for the pathology, but often these results fail to validate or only hold for the particular cancer at hand. In this study, we investigate a robust method for identifying cancer-related genes. Two criteria are used: a gene must show functional changes among clinically different groups of patients, and the gene must have clinical relevance. These conditions are assessed through a differential network analysis and by modelling patient survival times. Preliminary results are shown from a neuroblastoma and breast cancer dataset.\nEvent: ISMB/CAMDA 2018, Chicago, USA\nSlides\n\nQ&A starts at 16:20"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nA pseudo-value regression approach for differential network analysis of co-expression data\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA novel probabilistic generator for large-scale gene association networks\n\n\n\n\n\n\n\n\n\nNov 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Analysis of Gene Expression Data Incorporating Tumor Purity Information\n\n\n\n\n\n\n\n\n\nAug 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeqNet: An R Package for Generating Gene-Gene Networks and Simulating RNA-Seq Data\n\n\n\n\n\n\n\n\n\nJul 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrating gene regulatory pathways into differential network analysis of gene expression data\n\n\n\n\n\n\n\n\n\nApr 2, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting survival times for neuroblastoma patients using RNA-seq expression profiles\n\n\n\n\n\n\n\n\n\nMay 30, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnraveling bacterial fingerprints of city subways from microbiome 16S gene profiles\n\n\n\n\n\n\n\n\n\nMay 22, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github\n  \n  \n     Google Scholar\n  \n\n      \nBiostatistician working in genomics and clinical research, currently lead analyst for studies of immunologic response to disease and vaccines through the analysis of transcriptomics, genomics, proteomics, metabolomics, and lipidomics data. Highly skilled in developing bioinformatics analysis workflows, building predictive models, and designing simulation studies. Key role is as communicator of statistical results and connecting the findings to the biological question at hand."
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "About",
    "section": "Interests",
    "text": "Interests\n\nStatistical methods for -omics data\nGraphical models and differential network analysis\nPredictive modeling\nHigh-dimensional data analysis\nStatistical computing\n\nPast research has focused on RNA-seq data (gene expression), developing methods in the areas of graphical modeling, prediction models, and survival analysis. Typical research questions included: How can gene expression be used to improve prediction of survival in cancer patients? Do gene regulatory networks differ in high-risk vs. low-risk patients, and what do those differences tell us about the underlying disease?\nI find that simulations are an indispensable tool for modern research. Aside from allowing us to assess model performance, creating a simulation forces us to think deeply about the data generating process and the context of the problem at hand–this is a process that has often lead me to new insights."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts [under construction]",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nOverfitting in the search for biomarkers\n\n\n\n\n\n\n\nStatistical modeling\n\n\nHigh-dimensional\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nTyler Grimes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nB.S. in Mathematics | UCF, Orlando FL | 2010 – 2014\nMy undergraduate started off in computer science. Going into college I wasn’t sure what major to pursue. I never programmed before, but CS sounded like a challenging-enough major that would lead to a good career. I didn’t know anything about CS, and I certainly didn’t expect to have a passion for it, but I was in for a surprise. My first semester Intro to CS course hooked me right away. Learning C was tough, but it was like playing with LEGOs and unlimited resources. I could build anything I could imagine. I found myself spending hours coding, fixing bugs, coding, on repeat. CS I and II led to even more tools to build from–algorithms and data structures–and more interesting problems to solve. Programming problems would linger in the back of my mind, unknown to me, and a solution would pop up days later. I never experienced that before. The more courses I took, the more entrenched I became. From compilers–learning how to build a programming language from scratch–to computer architecture and object-oriented programming, I was enthralled.\nOne thing about learning programming was how complementary it was to learning math. Some of the first proofs I saw were in CS, like proving Dijkstra’s shortest path algorithm, but even thinking about functions in C carried over to functions in algebra. Math was always my strong suit growing up, but I only took up to pre-cal in high school and my overall understanding was sub par. While going through the CS program at UCF I took as many math courses as I could to build a better foundation. I started with algebra, trig, and pre-calc my first year and continued through calc I-III, ODE, and linear algebra after that. Calc I was the hardest–throughout the semester I always felt a week behind–but by the end it all made sense. The other calc courses were a relative breeze, albeit fueled by lots of practice. That was, until, linear algebra II. The first proof-based math course I took, it hit like a brick wall. Rather than feeling a week behind, it wasn’t until the end of the course when things began to click. Definitions were important (a revelation!). By the end, I realized almost all the theorems and proofs were essentially “by definition” … it just required connecting a few dots. Grade-wise, I did the worst in that course than any other. Learning-wise, this was my Mount Everest; and I reached the summit. I wouldn’t say it got me hooked like that Intro to CS course, but by the end I saw a new dimension to math that I didn’t know existed.\nBy my junior year I started learning about ML. I began to realize that ML was all “just” math, statistics, and some coding. It was a fascinating combination of CS and math, though, with important real-world applications. One of the first examples I was exposed to was identifying subtypes of breast cancer using ML, with each subtype having a different prognosis and implication for treatment. This was a crossroad where I began to strongly consider going to grad school for statistics–with the idea of pursing this new-to-me field of ML–but there was a catch. Most grad programs for statistics required advanced calc as a pre-req. I had one year to go, and there was exactly one advanced calc course offered the following fall semester. If I wanted to apply to grad school, I’d have to take that course. But, as fate would have it, the schedule conflicted with a required CS course that was also only offered in the fall. If I wanted to graduate on time, I’d have to choose between finishing the CS degree or switching completely over to math with plans to apply to grad school.\nThe year that followed was packed with math courses: abstract algebra, advance calc I-II, complex analysis, graph theory, optimization and so much more. These proved to be far more interesting that I thought they’d be (and in another life I may have pursued one of these instead of statistics), but it was a lot to consume in a single year. I was pulled in many directions, with new interests in math while still drawn towards CS/ML. In the end, the CS/ML interests won. But my experience during this final year resulted in a strong foundation in math that I could step on in addition to the CS background I built up the years prior. This CS/math background would prove to be a perfect foundation for the path I heading down.\nM.S. in Mathematics (concentration in statistics) | UNF, Jacksonville FL | 2014 – 2016\nAlthough this was a math program, the core courses were a mix of math and stat and all the electives were stat. I went into this program knowing almost no statistics, so the summer prior to starting I decided to work through the “Head First Statistics: A Brain-Friendly Guide” book. This was useful for consolidating some of the concepts I learned in stat I and II at UCF (while I did take some stat courses, these never really “clicked” for me). My memory of the first semester of this program is dominated by one course: design of experiments. We immediately got into multivariate regression modeling using matrix notation and it took me a while to catch up. (The stat methods course I was taking the same semester wouldn’t get to this for a few weeks.) In hind-sight it was a lot like the linear algebra II course I took at UCF–it wasn’t until the end of the semester when things started to click. Like math, you can do a lot in statistics without really understanding what’s going on conceptually. But this program forced the conceptual understanding on me right out of the gate, which I am thankful for. It allowed me the entire two years to digest and internalize a lot of statistical theory.\nDuring this program was the first time I used R. Wow, this a strange language coming form a CS background (I mean, who starts an index for an array at 1 and not 0?). I don’t remember how I got started using R–the courses in the UNF program used SAS (which was an even stranger venture) and I didn’t have any R books–but early on I started diving into it on my own. Since I got into statistics through ML, I also learned some Python along the way (which made a lot more sense compared to R!), but by the end of the master’s program I became the most proficient in R.\nPart of my goal with pursing a master’s was to decide if I wanted to do a doctorate. I had applied to some jobs during my second year at UNF, but none of those panned out. I wasn’t too bothered though because I was leaning toward continuing into a Ph.D. program and pursuing a job in academia after that. The motivations that originally pushed me to grad school were continuing to grow, and what I learned at UNF felt like a stepping stone: I learned enough to know how little I knew, but I began to see how much more there was to know.\nPh.D. in Biostatistics | UF, Gainesville FL | 2016 – 2020\nJust two hours from UNF, I landed in Gainesville–my new home for the next four years. Biostatistics was not a field I knew existed even a year prior, but, strangely, it was the field I was to become an “expert” in. It did sound perfect though–a blend of statistics and biology with a hefty dash of CS–all of my interests merged together.\nI would have a lot to catch up on with biology though. Freshmen Biology 1 was the only background I had. That course was fascinating, but it ended up being a lot of memorization. I remember spending an inordinate amount of time in the library studying, trying to understand what was going on, but ultimately relying on rote memorization. I went in interested in biology and medicine, but that course steered me away. One of the things I liked most about CS was that, once you learned and understood some basic ideas, you could solve all sorts of problems just from those building blocks. The same was true for math and statistics–memorizing wasn’t so crucial when results could be re-derived when needed. It’s not to say memorization isn’t important in all these fields, but biology just didn’t have the same core concepts that you could build from.\nFunny enough, it was math and CS that drew me back to biology. The first example I saw of ML was in biology: finding cancer subtypes using clustering algorithms on gene expression data. I started seeing ML as a way of exploring biology without all the memorization, a way to utilize the building blocks of math and CS to solve interesting problems in biology. That was the vision that set me down this path.\nAt UF my research was focused on the analysis of gene-gene association networks using gene expression measured from RNA-sequencing. Of course, my first hurdle was to understand what in the world a “gene association network” was (and “RNA-sequencing”). It made some sense when thinking about these networks from either a biological or statistical perspective. Biologically, it’s a graph–think nodes and edges, not a graph of a function–where two genes are connected by an edge if the RNA or gene product of one regulated the expression of the other. Statistically, the expression of each gene could be thought of as random variables, and two genes are connected if their random variables are dependent. This is a simplification, and the details are complicated, but the point is that the idea from either perspective was understandable.\nThe challenge was combining the two perspectives: what exactly does statistical dependence observed in expressed genes tell us about the underlying gene regulation? When first reading through the literature, I always felt like I was missing something. I knew enough statistical theory to be skeptical about how much we could interpret from those models, but I lacked the understanding of biology to convince myself whether the interpretations I was reading were appropriate.\nIt wasn’t until much later that I learned my skepticism was correct; the relationship between what we observe through RNA-sequencing and what is happening biologically is blurred due to differences in the rates of transcription, translation, and degradation of RNA and proteins, and the exact relationship depends on the organism and cell type being investigated. To make matters worse, the statistical models used to assess the dependence structure from gene expression data relied on faulty assumptions. Now, false assumptions are not necessarily bad–some assumptions are less important than others, and some important assumptions may be wrong but not far from the truth–but I showed through simulation studies that the assumptions in this case were important if we wanted to have a reliable biological interpretation. One paper I wrote on this proposed a statistical framework for integrating more biological knowledge into the analysis and allows for a wide variety of statistical models to be used, as that is a key factor in being able to answer biological questions. In another paper, I proposed a probabilistic model for generating gene-gene co-expression networks and simulated gene expression data from those networks, and, compared to existing models, this generator can better match the variety we see across different organisms and tissue types. This allows for more comprehensive simulation studies to be conducted for benchmarking algorithms that infer these gene-gene co-expression networks.\nBy the end of my Ph.D. the idealized vision I started off with of using ML to solve complex biological questions was still alive, but I saw a lot more of the limitations. For every dozen papers that used a fancy new ML approach, maybe one actually answered a biological question. In retrospect, I was approaching a fork in the road. I was heading down a new direction with a mindset less set on the idea that ML was enough to solve important biological questions, to one that realized the importance of well designed experiments and good data. It seems obvious to say now, but with so much data available today it feels like the answer is already out there waiting to be extracted; just feed it all into the right ML algorithm, crank the wheel, and pull out the all the answers. But that’s not how I see it anymore. The real ML solution is more like a tool that, given a biological question, can identify the right experiment needed and either offer that study design and analysis plan as the answer or find data from such experiments that have already been conducted. (Sounds an awful lot like what a Biologist does!) If I wanted to make real impact, I’d need to get closer to the studies and experiments that were being done."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nResearch Assistant | UF, Gainesville FL | 2016 – 2020\n[under construction]\nStatistician | VA, Brain Rehabilitation Research Center | 2018 – 2020\n[under construction]\nAssistant Professor of Statistics | UNF, Jacksonville FL | Aug. 2020 – May 2023\n[under construction]\nSenior Bioinformatics Analyst | The Emmes Company, Remote | Dec. 2021 – current\n[under construction]"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Tyler Grimes",
    "section": "Interests",
    "text": "Interests\n\nStatistical methods for -omics data\nGraphical models and differential network analysis\nPredictive modeling\nHigh-dimensional data analysis\nStatistical computing\n\nPast research has focused on RNA-seq gene expression data, developing methods in the areas of graphical modeling, prediction, and survival analysis. Typical research questions included: How can gene expression be used to improve prediction of survival in cancer patients? Do gene regulatory networks differ in high-risk vs. low-risk patients, and what do those differences tell us about the underlying disease?\nI find that simulations are an indispensable tool for modern research. Aside from allowing us to assess model performance, creating a simulation forces us to think deeply about the data generating process and the context of the problem at hand–this is a process that has often lead me to new insights."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Tyler Grimes",
    "section": "Education",
    "text": "Education\nB.S. in Mathematics | UCF, Orlando FL | 2010 – 2014\nMy undergraduate started off in computer science. Going into college I wasn’t sure what major to pursue. I never programmed before, but CS sounded like a challenging-enough major that would lead to a good career. I didn’t know anything about CS, and I certainly didn’t expect to have a passion for it, but I was in for a surprise. My first semester Intro to CS course hooked me right away. Learning C was tough, but it was like playing with LEGOs and unlimited resources. I could build anything I could imagine. I found myself spending hours coding, fixing bugs, coding, on repeat. CS I and II led to even more tools to build from–algorithms and data structures–and more interesting problems to solve. Programming problems would linger in the back of my mind, unknown to me, and a solution would pop up days later. I never experienced that before. The more courses I took, the more entrenched I became. From compilers–learning how to build a programming language from scratch–to computer architecture and object-oriented programming, I was enthralled.\nOne thing about learning programming was how complementary it was to learning math. Some of the first proofs I saw were in CS, like proving Dijkstra’s shortest path algorithm, but even thinking about functions in C carried over to functions in algebra. Math was always my strong suit growing up, but I only took up to pre-cal in high school and my overall understanding was sub par. While going through the CS program at UCF I took as many math courses as I could to build a better foundation. I started with algebra, trig, and pre-calc my first year and continued through calc I-III, ODE, and linear algebra after that. Calc I was the hardest–throughout the semester I always felt a week behind–but by the end it all made sense. The other calc courses were a relative breeze, albeit fueled by lots of practice. That was, until, linear algebra II. The first proof-based math course I took, it hit like a brick wall. Rather than feeling a week behind, it wasn’t until the end of the course when things began to click. Definitions were important (a revelation!). By the end, I realized almost all the theorems and proofs were essentially “by definition” … it just required connecting a few dots. Grade-wise, I did the worst in that course than any other. Learning-wise, this was my Mount Everest; and I reached the summit. I wouldn’t say it got me hooked like that Intro to CS course, but by the end I saw a new dimension to math that I didn’t know existed.\nBy my junior year I started learning about ML. I began to realize that ML was all “just” math, statistics, and some coding. It was a fascinating combination of CS and math, though, with important real-world applications. One of the first examples I was exposed to was identifying subtypes of breast cancer using ML, with each subtype having a different prognosis and implication for treatment. This was a crossroad where I began to strongly consider going to grad school for statistics–with the idea of pursing this new-to-me field of ML–but there was a catch. Most grad programs for statistics required advanced calc as a pre-req. I had one year to go, and there was exactly one advanced calc course offered the following fall semester. If I wanted to apply to grad school, I’d have to take that course. But, as fate would have it, the schedule conflicted with a required CS course that was also only offered in the fall. If I wanted to graduate on time, I’d have to choose between finishing the CS degree or switching completely over to math with plans to apply to grad school.\nThe year that followed was packed with math courses: abstract algebra, advance calc I-II, complex analysis, graph theory, optimization and so much more. These proved to be far more interesting that I thought they’d be (and in another life I may have pursued one of these instead of statistics), but it was a lot to consume in a single year. I was pulled in many directions, with new interests in math while still drawn towards CS/ML. In the end, the CS/ML interests won. But my experience during this final year resulted in a strong foundation in math that I could step on in addition to the CS background I built up the years prior. This CS/math background would prove to be a perfect foundation for the path I heading down.\nM.S. in Mathematics (concentration in statistics) | UNF, Jacksonville FL | 2014 – 2016\nAlthough this was a math program, the core courses were a mix of math and stat and all the electives were stat. I went into this program knowing almost no statistics, so the summer prior to starting I decided to work through the “Head First Statistics: A Brain-Friendly Guide” book. This was useful for consolidating some of the concepts I learned in stat I and II at UCF (while I did take some stat courses, these never really “clicked” for me). My memory of the first semester of this program is dominated by one course: design of experiments. We immediately got into multivariate regression modeling using matrix notation and it took me a while to catch up. (The stat methods course I was taking the same semester wouldn’t get to this for a few weeks.) In hind-sight it was a lot like the linear algebra II course I took at UCF–it wasn’t until the end of the semester when things started to click. Like math, you can do a lot in statistics without really understanding what’s going on conceptually. But this program forced the conceptual understanding on me right out of the gate, which I am thankful for. It allowed me the entire two years to digest and internalize a lot of statistical theory.\nDuring this program was the first time I used R. Wow, this a strange language coming form a CS background (I mean, who starts an index for an array at 1 and not 0?). I don’t remember how I got started using R–the courses in the UNF program used SAS (which was an even stranger venture) and I didn’t have any R books–but early on I started diving into it on my own. Since I got into statistics through ML, I also learned some Python along the way (which made a lot more sense compared to R!), but by the end of the master’s program I became the most proficient in R.\nPart of my goal with pursing a master’s was to decide if I wanted to do a doctorate. I had applied to some jobs during my second year at UNF, but none of those panned out. I wasn’t too bothered though because I was leaning toward continuing into a Ph.D. program and pursuing a job in academia after that. The motivations that originally pushed me to grad school were continuing to grow, and what I learned at UNF felt like a stepping stone: I learned enough to know how little I knew, but I began to see how much more there was to know.\nPh.D. in Biostatistics | UF, Gainesville FL | 2016 – 2020\nJust two hours from UNF, I landed in Gainesville–my new home for the next four years. Biostatistics was not a field I knew existed even a year prior, but, strangely, it was the field I was to become an “expert” in. It did sound perfect though–a blend of statistics and biology with a hefty dash of CS–all of my interests merged together.\nI would have a lot to catch up on with biology though. Freshmen Biology 1 was the only background I had. That course was fascinating, but it ended up being a lot of memorization. I remember spending an inordinate amount of time in the library studying, trying to understand what was going on, but ultimately relying on rote memorization. I went in interested in biology and medicine, but that course steered me away. One of the things I liked most about CS was that, once you learned and understood some basic ideas, you could solve all sorts of problems just from those building blocks. The same was true for math and statistics–memorizing wasn’t so crucial when results could be re-derived when needed. It’s not to say memorization isn’t important in all these fields, but biology just didn’t have the same core concepts that you could build from.\nFunny enough, it was math and CS that drew me back to biology. The first example I saw of ML was in biology: finding cancer subtypes using clustering algorithms on gene expression data. I started seeing ML as a way of exploring biology without all the memorization, a way to utilize the building blocks of math and CS to solve interesting problems in biology. That was the vision that set me down this path.\nAt UF my research was focused on the analysis of gene-gene association networks using gene expression measured from RNA-sequencing. Of course, my first hurdle was to understand what in the world a “gene association network” was (and “RNA-sequencing”). It made some sense when thinking about these networks from either a biological or statistical perspective. Biologically, it’s a graph–think nodes and edges, not a graph of a function–where two genes are connected by an edge if the RNA or gene product of one regulated the expression of the other. Statistically, the expression of each gene could be thought of as random variables, and two genes are connected if their random variables are dependent. This is a simplification, and the details are complicated, but the point is that the idea from either perspective was understandable.\nThe challenge was combining the two perspectives: what exactly does statistical dependence observed in expressed genes tell us about the underlying gene regulation? When first reading through the literature, I always felt like I was missing something. I knew enough statistical theory to be skeptical about how much we could interpret from those models, but I lacked the understanding of biology to convince myself whether the interpretations I was reading were appropriate.\nIt wasn’t until much later that I learned my skepticism was correct; the relationship between what we observe through RNA-sequencing and what is happening biologically is blurred due to differences in the rates of transcription, translation, and degradation of RNA and proteins, and the exact relationship depends on the organism and cell type being investigated. To make matters worse, the statistical models used to assess the dependence structure from gene expression data relied on faulty assumptions. Now, false assumptions are not necessarily bad–some assumptions are less important than others, and some important assumptions may be wrong but not far from the truth–but I showed through simulation studies that the assumptions in this case were important if we wanted to have a reliable biological interpretation. One paper I wrote on this proposed a statistical framework for integrating more biological knowledge into the analysis and allows for a wide variety of statistical models to be used, as that is a key factor in being able to answer biological questions. In another paper, I proposed a probabilistic model for generating gene-gene co-expression networks and simulated gene expression data from those networks, and, compared to existing models, this generator can better match the variety we see across different organisms and tissue types. This allows for more comprehensive simulation studies to be conducted for benchmarking algorithms that infer these gene-gene co-expression networks.\nBy the end of my Ph.D. the idealized vision I started off with of using ML to solve complex biological questions was still alive, but I saw a lot more of the limitations. For every dozen papers that used a fancy new ML approach, maybe one actually answered a biological question. In retrospect, I was approaching a fork in the road. I was heading down a new direction with a mindset less set on the idea that ML was enough to solve important biological questions, to one that realized the importance of well designed experiments and good data. It seems obvious to say now, but with so much data available today it feels like the answer is already out there waiting to be extracted; just feed it all into the right ML algorithm, crank the wheel, and pull out the all the answers. But that’s not how I see it anymore. The real ML solution is more like a tool that, given a biological question, can identify the right experiment needed and either offer that study design and analysis plan as the answer or find data from such experiments that have already been conducted. (Sounds an awful lot like what a Biologist does!) If I wanted to make real impact, I’d need to get closer to the studies and experiments that were being done."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Tyler Grimes",
    "section": "Experience",
    "text": "Experience\nResearch Assistant | UF, Gainesville FL | 2016 – 2020\n[under construction]\nStatistician | VA, Brain Rehabilitation Research Center | 2018 – 2020\n[under construction]\nAssistant Professor of Statistics | UNF, Jacksonville FL | Aug. 2020 – May 2023\n[under construction]\nSenior Bioinformatics Analyst | The Emmes Company, Remote | Dec. 2021 – current\n[under construction]"
  },
  {
    "objectID": "index-old.html",
    "href": "index-old.html",
    "title": "Tyler Grimes",
    "section": "",
    "text": "No matching items"
  }
]